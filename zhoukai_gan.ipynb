{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Convolution2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Deterministic output.\n",
    "# Tired of seeing the same results every time? Remove the line below.\n",
    "np.random.seed(1000)\n",
    "\n",
    "# The results are a little better when the dimensionality of the random vector is only 10.\n",
    "# The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
    "randomDim = 100\n",
    "\n",
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(\n",
    "    Dense(\n",
    "        256,\n",
    "        input_dim=randomDim,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(0.2))\n",
    "generator.add(Dense(784, activation='tanh'))\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(\n",
    "    Dense(\n",
    "        1024,\n",
    "        input_dim=784,\n",
    "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "# Combined network\n",
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(randomDim, ))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam)\n",
    "\n",
    "dLosses = []\n",
    "gLosses = []\n",
    "\n",
    "\n",
    "# Plot the loss from each batch\n",
    "def plotLoss(epoch):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('images/loss_epoch_%06d.png' % epoch)\n",
    "\n",
    "\n",
    "# Create a wall of generated MNIST images\n",
    "def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generatedImages.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/generated_image_epoch_%06d.png' % epoch)\n",
    "\n",
    "\n",
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "def saveModels(epoch):\n",
    "    generator.save('models/generator_epoch_%06d.h5' % epoch)\n",
    "    discriminator.save('models/discriminator_epoch_%06d.h5' % epoch)\n",
    "\n",
    "\n",
    "def createNoise(generatedImages, thresholding, depth):\n",
    "    if depth > 20:\n",
    "        return generatedImages\n",
    "    noise_samples = discriminator.predict(generatedImages)\n",
    "    flag = False\n",
    "    for index, ns in enumerate(noise_samples):\n",
    "        if ns > thresholding:\n",
    "            flag = True\n",
    "            generatedImages[index] = generator.predict(\n",
    "                np.random.normal(0, 1, size=[1, randomDim]))\n",
    "    if flag == True:\n",
    "        generatedImages = createNoise(generatedImages, thresholding, depth + 1)\n",
    "    return generatedImages\n",
    "\n",
    "\n",
    "def set_step(epoch):\n",
    "    dloss_npy = np.load('Loss/dynamic_D_loss.npy')\n",
    "    gloss_npy = np.load('Loss/dynamic_G_loss.npy')\n",
    "    loss_all = dloss_npy[-1] + gloss_npy[-1]\n",
    "    d_step = dloss_npy[-1] / loss_all\n",
    "    g_step = gloss_npy[-1] / loss_all\n",
    "    return int(d_step * 20), int(g_step * 20)\n",
    "\n",
    "\n",
    "def train(epochs=1, batchSize=128, thresholding=0.95):\n",
    "    batchCount = X_train.shape[0] / batchSize\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.exists('Loss'):\n",
    "        os.makedirs('Loss')\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        print('-' * 15, 'Epoch %d' % e, '-' * 15)\n",
    "        #tq = tqdm(range(int(batchCount)))\n",
    "        #for _ in tqdm(range(0,int(batchCount),10)):\n",
    "        dloss_l = []\n",
    "        gloss_l = []\n",
    "\n",
    "        if e <= 20:\n",
    "            d_step, g_step = 10, 10\n",
    "        else:\n",
    "            d_step, g_step = set_step(e)\n",
    "        for _ in tqdm(range(0, int(batchCount), 20)):\n",
    "            # Get a random set of input noise and images\n",
    "            for i in range(d_step):\n",
    "                noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "                imageBatch = X_train[np.random.randint(\n",
    "                    0, X_train.shape[0], size=batchSize)]\n",
    "                #import pdb;pdb.set_trace()\n",
    "                # Generate fake MNIST images\n",
    "                generatedImages = generator.predict(noise)\n",
    "\n",
    "                generatedImages = createNoise(generatedImages, thresholding, 1)\n",
    "                #import pdb;pdb.set_trace()\n",
    "\n",
    "                #print(np.shape(imageBatch), np.shape(generatedImages))\n",
    "                X = np.concatenate([imageBatch, generatedImages])\n",
    "                #import pdb;pdb.set_trace()\n",
    "                # Labels for generated and real data\n",
    "                yDis = np.zeros(2 * batchSize)\n",
    "                # One-sided label smoothing\n",
    "                yDis[:batchSize] = 1\n",
    "\n",
    "                # Train discriminator\n",
    "                discriminator.trainable = True\n",
    "                dloss = discriminator.train_on_batch(X, yDis)\n",
    "                dloss_l.append(dloss)\n",
    "            # Train generator\n",
    "            discriminator.trainable = False\n",
    "            for j in range(g_step):\n",
    "                noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "                yGen = np.ones(batchSize)\n",
    "                gloss = gan.train_on_batch(noise, yGen)\n",
    "                gloss_l.append(gloss)\n",
    "        # Store loss of most recent batch from this epoch\n",
    "        dLosses.append(np.mean(dloss_l))\n",
    "        gLosses.append(np.mean(gloss_l))\n",
    "        print(\"D_loss:\", dLosses[e - 1])\n",
    "        print(\"G_loss:\", gLosses[e - 1])\n",
    "        np.save(\"Loss/dynamic_D_loss.npy\", dLosses)\n",
    "        np.save(\"Loss/dynamic_G_loss.npy\", gLosses)\n",
    "        if e == 1 or e % 10 == 0:\n",
    "            plotGeneratedImages(e)\n",
    "            saveModels(e)\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            plotLoss(e)\n",
    "\n",
    "    # Plot losses from every epoch\n",
    "    plotLoss(e)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(20000, 128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
